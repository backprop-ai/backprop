{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cutting-ebony",
   "metadata": {},
   "source": [
    "# Finetuning for Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "verbal-delivery",
   "metadata": {},
   "outputs": [],
   "source": [
    "import backprop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "violent-civilization",
   "metadata": {},
   "source": [
    "### Data prep\n",
    "\n",
    "When you're finetuning a model, you're going to need to provide it with some data.\n",
    "\n",
    "The benefit of finetuning is that the amount of data you need is substantially less than you'd need for training a new model from scratch.\n",
    "\n",
    "Here, we'll be using the IMDb movie review dataset, and training a model to classify sentiment.\n",
    "\n",
    "This dataset has movie reviews, along with an associated label indicating whether or not that review was positive or negative in tone. For this proof-of-concept approach, we'll get a working model trained on just 1000 examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "raised-configuration",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset imdb (/home/lacava/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "comparative-parish",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 1,\n",
       " 'text': 'Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school life, such as \"Teachers\". My 35 years in the teaching profession lead me to believe that Bromwell High\\'s satire is much closer to reality than is \"Teachers\". The scramble to survive financially, the insightful students who can see right through their pathetic teachers\\' pomp, the pettiness of the whole situation, all remind me of the schools I knew and their students. When I saw the episode in which a student repeatedly tried to burn down the school, I immediately recalled ......... at .......... High. A classic line: INSPECTOR: I\\'m here to sack one of your teachers. STUDENT: Welcome to Bromwell High. I expect that many adults of my age think that Bromwell High is far fetched. What a pity that it isn\\'t!'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "designed-winter",
   "metadata": {},
   "source": [
    "One thing to note about this dataset's training set is that it's ordered: the first half is entirely **positive** reviews (`label` is `1`), and the latter half entirely **negative** (`label` is `0`) reviews.\n",
    "\n",
    "Since we're just running a demo here, we're not going to use all 25000 examples in the set. We'll take 500 from that front half, and 500 from the back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "optimum-battle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For readability, we'll map the label numbers to strings.\n",
    "# This isn't necessary, but it looks nicer on output.\n",
    "def label_to_sentiment(label):\n",
    "    if label == 0:\n",
    "        return \"negative\"\n",
    "    else:\n",
    "        return \"positive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "golden-statement",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = []\n",
    "outputs = []\n",
    "for i in range(500):\n",
    "    inputs.append(dataset[\"train\"][i][\"text\"])\n",
    "    outputs.append(label_to_sentiment(dataset[\"train\"][i][\"label\"]))\n",
    "    inputs.append(dataset[\"train\"][-i][\"text\"])\n",
    "    outputs.append(label_to_sentiment(dataset[\"train\"][-i][\"label\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aquatic-stereo",
   "metadata": {},
   "source": [
    "### Finetuning\n",
    "\n",
    "Now that we've got our data prepared, we're ready to train.\n",
    "\n",
    "All we need to do is choose our task, supply it with the model we're training, and call `.finetune()` with our inputs and outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "stone-robinson",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: None, using: 0 TPU cores\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding the optimal batch size...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch size 2 succeeded, trying batch size 4\n",
      "Batch size 4 succeeded, trying batch size 8\n",
      "Batch size 8 succeeded, trying batch size 16\n",
      "Batch size 16 succeeded, trying batch size 32\n",
      "Batch size 32 failed, trying batch size 16\n",
      "Finished batch size finder, will continue with full run using batch size 16\n",
      "Restored states from the checkpoint file at /home/lacava/Documents/backprop/backprop/examples/scale_batch_size_temp_model.ckpt\n",
      "GPU available: True, used: True\n",
      "TPU available: None, using: 0 TPU cores\n",
      "\n",
      "  | Name  | Type                           | Params\n",
      "---------------------------------------------------------\n",
      "0 | model | XLNetForSequenceClassification | 117 M \n",
      "---------------------------------------------------------\n",
      "117 M     Trainable params\n",
      "0         Non-trainable params\n",
      "117 M     Total params\n",
      "469.242   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validation sanity check', layout=Layout…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c526e67841b4672bc2150f9b511c227",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training finished! Save your model for later with backprop.save or upload it with backprop.upload\n"
     ]
    }
   ],
   "source": [
    "tc = backprop.TextClassification(backprop.models.XLNet)\n",
    "tc.finetune(inputs, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "personalized-camping",
   "metadata": {},
   "source": [
    "Great, that didn't take too long.\n",
    "\n",
    "Let's write some fake review and see how it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "subsequent-client",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_review = \"\"\"Backprop: The Movie had its highlights, but ultimately fell flat. \n",
    "            While AI is a fascinating concept, watching people sit and train models\n",
    "            in real-time just doesn't make for an exciting viewing experience. \n",
    "            This is made especially egregious by the four-and-a-half hour runtime.\"\"\"\n",
    "\n",
    "pos_review = \"\"\"Backprop: The Movie was an absolute pleasure from start to finish.\n",
    "             The writing was witty, the concept was engaging, and the music was beautiful.\n",
    "             There's just something about a well-trained model that ties a film together.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "answering-fabric",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to pad to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no padding.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'negative': 0.956028938293457, 'positive': 0.04397110268473625}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tc(neg_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cosmetic-secondary",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'negative': 0.010379660874605179, 'positive': 0.9896203279495239}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tc(pos_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southeast-costume",
   "metadata": {},
   "source": [
    "Perfect -- after just a bit of training, our model can correctly classify the reviews of *Backprop: The Movie* (coming soon).\n",
    "\n",
    "It's worth noting here that this is a binary classifier: our dataset had only two labels. Every review it was trained on is distinctly positive or negative, no nuance.\n",
    "\n",
    "Let's try a review with some grey area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "catholic-sheet",
   "metadata": {},
   "outputs": [],
   "source": [
    "mix_review = \"\"\"Backprop: The Movie was certainly an experimental film.\n",
    "                The slow sections were an absolute drag. This was made worse \n",
    "                by just how long this film is.\n",
    "                However, the idea was clever, and it was pretty well-written, even\n",
    "                if some of the actors weren't great.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "stunning-verification",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'negative': 0.3215426206588745, 'positive': 0.6784573793411255}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tc(mix_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "monthly-shopping",
   "metadata": {},
   "source": [
    "Aha!\n",
    "\n",
    "Now our results are substantially less polarized.\n",
    "\n",
    "Even though our dataset didn't have a 'neutral' label, we can still take the lack of confidence in calling the review `negative` or `positive` as an implied neutral label. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
