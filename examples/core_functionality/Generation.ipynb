{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kiri Core Example: Text Generation\n",
    "\n",
    "Generate text based on some provided input.\n",
    "\n",
    "The default behavior here is that of a standard instance of GPT-2 -- it'll continue writing based on whatever context you've written.\n",
    "\n",
    "Other generative models, such as T5, can be used as well. If you've trained a model, you can pass in the required tokenizer/model checkpoints and use generate for a variety of tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's the deal with all the parameters?\n",
    "\n",
    "Text generation has a *lot* of parameter options. Some tweaking may be needed for you to get optimal results for your use case. I'll cover what we make accessible, and how they can change generation. \n",
    "\n",
    "- `min_length`: Forces the model to continue writing until at least the supplied `min_length` is reached.\n",
    "---\n",
    "- `temperature`: Alters the probability distribution of the model's softmax. Raising this above 1.0 will lead to an increase in 'out there' token choices, that the model would ordinarily be less confident to select. Lowering it below 1.0 makes the distribution sharper, leading to 'safer' choices.\n",
    "---\n",
    "- `top_k`: Method of sampling in which the *K* most likely next words are identified, and the probability is redistributed among those *K*.\n",
    "---\n",
    "- `top_p`: Method of sampling in which a probability threshold *p* is chosen. The smallest possible set of words with a combined probability exceeding *p* are selected, and the probability is redistributed among that set.  \n",
    "---\n",
    "- `do_sample`: Determines whether or not sampling is performed.\n",
    "---\n",
    "- `repetition_penalty`: Adds a penalty to words that are present in the input context, and to words that are already included in the generated sequence.\n",
    "---\n",
    "- `length_penalty`: Penalty applied to the length of a generated sequence. Defaults to 1.0 (no penalty). Set to lower than 1.0 to get shorter sequences, or higher than 1.0 to get longer ones.\n",
    "---\n",
    "- `num_beams`: Number of beams to use in beam search. \n",
    "\n",
    "**Beam search** maintains `num_beams` different branches of word generation sequences, and returns the one with the highest overall probability. In practice, this is a way to ensure that the generator doesn't miss probable word sequences that may be obscured by an early low-probability word choice. \n",
    "\n",
    "Setting `num_beams` to 1 means no beam search will be used.\n",
    "\n",
    "---\n",
    "- `num_generations`: Number of times the generator will run on the given input. Will give you back a list of results from generation.\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you've got one, change it here.\n",
    "api_key = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kiri import Kiri\n",
    "\n",
    "# No model specification needed for plain GPT-2.\n",
    "if api_key:\n",
    "    kiri = Kiri(api_key=api_key)\n",
    "else:\n",
    "    kiri = Kiri(local=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Geralt knew the signs: the monster was a vampire that day; after the siege her companions'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The basic functionality, just picks up where you leave off.\n",
    "\n",
    "kiri.generate(\"Geralt knew the signs: the monster was a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supplying your own checkpoints\n",
    "\n",
    "As mentioned, the default generator is GPT-2.\n",
    "\n",
    "Let's try supplying another model -- one of Kiri's pretrained T5 models. I'll be using the same model that our Sentiment Detection and Summarization modules use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'remorse, disappointment, sadness'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_emote_model = \"kiri-ai/t5-base-qa-summary-emotion\"\n",
    "\n",
    "# Initialise Kiri with our model checkpoint\n",
    "kiri_t5 = Kiri(local=True, \n",
    "               generation_model=\"kiri-ai/t5-base-qa-summary-emotion\")\n",
    "\n",
    "\n",
    "# Our sentiment function automatically adds the 'emotion:' prefix.\n",
    "# As we're accessing the generator directly, we need to do it.\n",
    "input_text = \"\"\"emotion: This food was just not good.\n",
    "                Sorry, but you need to do better. \n",
    "                Really gross and undercooked.\"\"\"\n",
    "\n",
    "# We want \n",
    "kiri_t5.generate(input_text, do_sample=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
