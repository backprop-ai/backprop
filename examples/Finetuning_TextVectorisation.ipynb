{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning for Text Vectorisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> If you are new to text vectorisation be sure to look at the text vectorisation notebook first.\n",
    "\n",
    "Finetuning a text vectorisation task is mostly a matter of optimisation.\n",
    "\n",
    "Our supported text vectorisation models are applicable to multiple types of text vectorisation use cases: from detecting similar questions to finding paragraphs that contain answers to some question.\n",
    "\n",
    "However, you may be able to make your use case significantly more accurate with finetuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import backprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the data\n",
    "\n",
    "The most value for text vectorisation comes by using your own data for finetuning.\n",
    "\n",
    "For this example, we will be using the Quora duplicate questions dataset. One row of data contains two questions and whether they are duplicate or not.\n",
    "\n",
    "Finetuning for text vectorisation uses cosine similarity to compare how similar the vectors are. Therefore, we can score duplicates as `1.0` and non duplicates as `0.0`. Any value between 0 and 1 works, but this dataset does not contain more finegrained information.\n",
    "\n",
    "Our input data will be a list of question tuples (`[(q1, q2), (q3, q4)]`) and our output data will be a list of corresponding scores (`[0.0, 1.0]`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset quora (/home/kristo/.cache/huggingface/datasets/quora/default/0.0.0/2be517cf0ac6de94b77a103a36b141347a13f40637fbebaccb56ddbe397876be)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"quora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'is_duplicate': False,\n",
       " 'questions': {'id': [1, 2],\n",
       "  'text': ['What is the step by step guide to invest in share market in india?',\n",
       "   'What is the step by step guide to invest in share market?']}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'is_duplicate': True,\n",
       " 'questions': {'id': [15, 16],\n",
       "  'text': ['How can I be a good geologist?',\n",
       "   'What should I do to be a great geologist?']}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts1 = []\n",
    "texts2 = []\n",
    "similarity_scores = []\n",
    "\n",
    "num_positive = 0\n",
    "num_negative = 0\n",
    "\n",
    "for i in range(len(dataset[\"train\"])):\n",
    "    # Get 500 positive and 500 negative examples\n",
    "    similarity = 1.0 if dataset[\"train\"][i][\"is_duplicate\"] else 0.0\n",
    "    \n",
    "    if similarity == 1.0 and num_positive >= 500:\n",
    "        continue\n",
    "    else:\n",
    "        num_positive += 1\n",
    "        \n",
    "    if similarity == 0.0 and num_negative >= 500:\n",
    "        continue\n",
    "    else:\n",
    "        num_negative += 1\n",
    "    \n",
    "    questions = dataset[\"train\"][i][\"questions\"]\n",
    "    q1 = questions[\"text\"][0]\n",
    "    q2 = questions[\"text\"][1]\n",
    "    # Tuple\n",
    "    texts1.append(q1)\n",
    "    texts2.append(q2)\n",
    "    \n",
    "    similarity_scores.append(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('What is the step by step guide to invest in share market in india?',\n",
       " 'What is the step by step guide to invest in share market?',\n",
       " 0.0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts1[0], texts2[0], similarity_scores[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('How can I be a good geologist?',\n",
       " 'What should I do to be a great geologist?',\n",
       " 1.0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts1[7], texts2[7], similarity_scores[7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a good idea to keep the examples roughly balanced. Otherwise finetuning just makes the model more biased toward some score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning\n",
    "\n",
    "All we do is pass in our question pairs as input data and our similarity scores as output data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what model we can use for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name                     msmarco-distilroberta-base-v2\n",
      "Description              This English model is a standard distilroberta-base model from the Sentence Transformers repo, which has been trained on the MS MARCO dataset.\n",
      "Supported tasks          ['text-vectorisation']\n",
      "Finetunable tasks        ['text-vectorisation']\n",
      "----------\n",
      "Name                     distiluse-base-multilingual-cased-v2\n",
      "Description              This model is based off Sentence-Transformer's distiluse-base-multilingual-cased multilingual model that has been extended to understand sentence embeddings in 50+ languages.\n",
      "Supported tasks          ['text-vectorisation']\n",
      "Finetunable tasks        ['text-vectorisation']\n",
      "----------\n",
      "Name                     clip-vit-b32\n",
      "Alias                    clip\n",
      "Description              OpenAI's recently released CLIP model — when supplied with a list of labels and an image, CLIP can accurately predict which labels best fit the provided image.\n",
      "Supported tasks          ['image-classification', 'image-vectorisation', 'text-vectorisation', 'image-text-vectorisation']\n",
      "Finetunable tasks        ['image-vectorisation', 'text-vectorisation', 'image-text-vectorisation']\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "backprop.TextVectorisation.list_models(display=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: None, using: 0 TPU cores\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding the optimal batch size...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch size 2 succeeded, trying batch size 4\n",
      "Batch size 4 succeeded, trying batch size 8\n",
      "Batch size 8 succeeded, trying batch size 16\n",
      "Batch size 16 failed, trying batch size 8\n",
      "Finished batch size finder, will continue with full run using batch size 8\n",
      "Restored states from the checkpoint file at /home/kristo/Documents/backprop/examples/scale_batch_size_temp_model.ckpt\n",
      "GPU available: True, used: True\n",
      "TPU available: None, using: 0 TPU cores\n",
      "\n",
      "  | Name  | Type    | Params\n",
      "----------------------------------\n",
      "0 | model | STModel | 135 M \n",
      "----------------------------------\n",
      "135 M     Trainable params\n",
      "0         Non-trainable params\n",
      "135 M     Total params\n",
      "540.511   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validation sanity check', layout=Layout…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aed636d7a3c241c78bac8f65ef38222e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training finished! Save your model for later with backprop.save or upload it with backprop.upload\n"
     ]
    }
   ],
   "source": [
    "# Start a text vectorisation task with a text vectorisation model\n",
    "tv = backprop.TextVectorisation(\"distiluse-base-multilingual-cased-v2\")\n",
    "# Length here refers to number of tokens (1 token ~ 1 word)\n",
    "tv.finetune({\"texts1\": texts1, \"texts2\": texts2, \"similarity_scores\": similarity_scores}, max_length=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = tv(\"Where did Bill Gates go to school?\")\n",
    "q2 = tv(\"What school did Bill Gates go to?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7503934502601624"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "backprop.cosine_similarity(q1, q2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = tv(\"Where did Bill Gates go to school?\")\n",
    "q2 = tv(\"What company did Bill Gates found?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.43922990560531616"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "backprop.cosine_similarity(q1, q2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = tv(\"Where did Bill Gates go to school?\")\n",
    "q2 = tv(\"How big is the moon?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.043116070330142975"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "backprop.cosine_similarity(q1, q2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the most similar questions get the highest score while the least similar questions get the lowest score."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
