{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backprop Core Example: Text Generation\n",
    "\n",
    "Generate text based on some provided input.\n",
    "\n",
    "The default behavior here is that of a standard instance of GPT-2 -- it'll continue writing based on whatever context you've written.\n",
    "\n",
    "Other generative models, such as T5, can be used as well. If you've trained a model, you can pass in the required tokenizer/model checkpoints and use generate for a variety of tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's the deal with all the parameters?\n",
    "\n",
    "Text generation has a *lot* of parameter options. Some tweaking may be needed for you to get optimal results for your use case. I'll cover what we make accessible, and how they can change generation. \n",
    "\n",
    "- `min_length`: Forces the model to continue writing until at least the supplied `min_length` is reached.\n",
    "\n",
    "---\n",
    "- `temperature`: Alters the probability distribution of the model's softmax. Raising this above 1.0 will lead to an increase in 'out there' token choices, that the model would ordinarily be less confident to select. Lowering it below 1.0 makes the distribution sharper, leading to 'safer' choices.\n",
    "\n",
    "---\n",
    "- `top_k`: Method of sampling in which the *K* most likely next words are identified, and the probability is redistributed among those *K*.\n",
    "\n",
    "---\n",
    "- `top_p`: Method of sampling in which a probability threshold *p* is chosen. The smallest possible set of words with a combined probability exceeding *p* are selected, and the probability is redistributed among that set.  \n",
    "\n",
    "---\n",
    "- `do_sample`: Determines whether or not sampling is performed.\n",
    "\n",
    "---\n",
    "- `repetition_penalty`: Adds a penalty to words that are present in the input context, and to words that are already included in the generated sequence.\n",
    "\n",
    "---\n",
    "- `length_penalty`: Penalty applied to the length of a generated sequence. Defaults to 1.0 (no penalty). Set to lower than 1.0 to get shorter sequences, or higher than 1.0 to get longer ones.\n",
    "\n",
    "---\n",
    "- `num_beams`: Number of beams to use in beam search. \n",
    "\n",
    "**Beam search** maintains `num_beams` different branches of word generation sequences, and returns the one with the highest overall probability. In practice, this is a way to ensure that the generator doesn't miss probable word sequences that may be obscured by an early low-probability word choice. \n",
    "\n",
    "Setting `num_beams` to 1 means no beam search will be used.\n",
    "\n",
    "---\n",
    "- `num_generations`: Number of times the generator will run on the given input. Will give you back a list of results from generation.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leave it as None to run locally\n",
    "api_key = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import backprop\n",
    "\n",
    "# No model specification needed for plain GPT-2.\n",
    "tg = backprop.TextGeneration(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Geralt knew the signs: the monster was a killer, his wounds, and the girl was to blame. Geralt couldn't escape her; only time could prove his guilt. He turned and fled back out of sight, but not before telling\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The basic functionality, just picks up where you leave off.\n",
    "tg(\"Geralt knew the signs: the monster was a\", temperature=1.2, max_length=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supplying your own checkpoints\n",
    "\n",
    "As mentioned, the default generator is GPT-2.\n",
    "\n",
    "Let's try supplying another model -- one of Backprop's pretrained T5 models. I'll be using the same model that our Sentiment Detection and Summarisation modules use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'remorse'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialise Text Generation with our model checkpoint\n",
    "\n",
    "tg_t5 = backprop.TextGeneration(model=\"t5-base-qa-summary-emotion\", api_key=api_key)\n",
    "\n",
    "# Our sentiment function automatically adds the 'emotion:' prefix.\n",
    "# As we're accessing the generator directly, we need to do it.\n",
    "input_text = \"\"\"emotion: This food was just not good.\n",
    "                Sorry, but you need to do better. \n",
    "                Really gross and undercooked.\"\"\"\n",
    "\n",
    "\n",
    "tg_t5(input_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetuning\n",
    "\n",
    "As you just saw, text generation can be extremely powerful. The above model has been finetuned for conversational question answering, emotion detection and text summarisation.\n",
    "\n",
    "As text generation models just take some text as input and produce some text as output, it makes them very versatile.\n",
    "\n",
    "With further finetuning, it is possible to solve any text based task. Check out our finetuning notebook for an example!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
